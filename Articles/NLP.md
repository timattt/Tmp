# Эмбединги

## Без нейросетей

* смысл слова определеяется контекстом
* идем окном
* строим матрицу встречаемости слов, iое слово с jым
* можно понизить размерность
* еще можно сингулярно разложить. Тогда строки первой матрицы и строки последней - искомые эмбединги

## С нейросетями

* обучаем нейросеть - на вход onehot слово - на выходе тоже onehot слово. Фактически это задача классификации 

![image](https://github.com/timattt/Tmp/assets/25401699/3ed4276a-1107-4b4f-bcac-839f275cdd8d)

# seq2seq

## Encoder-decoder

![image](https://github.com/timattt/Tmp/assets/25401699/a1f2a5fc-8fbe-438a-8713-65aad2eedeff)

* контекст - эмбединг всего предложения, который передается от энкодера декодеру
* в конце последовательности энкодера - токен конца строки
* в начальном состоянии декодера на входе - токен начала строки
* обучение происзодит путем - teacher-forcing - в декодере подаем правильное значение вместо предсказанного

# BLEU

![image](https://github.com/timattt/Tmp/assets/25401699/7a182c6c-c7ed-4800-a321-b4601c0c6e95)

# attention

* энкодер-декобер забывает о начальных токенах
* аттеншн может быть любой функцией (напр, скалярным произведением)

![image](https://github.com/timattt/Tmp/assets/25401699/8bb9f4d2-365d-4d2b-941a-0729c6b2c2fb)

## self-attention

![image](https://github.com/timattt/Tmp/assets/25401699/1f49b084-3fb6-4ed6-a071-a090777fb928)
